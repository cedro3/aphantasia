{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "movie",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "f3Sj0fxmtw6K",
        "YdVubN0vb3TU"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedro3/aphantasia/blob/master/movie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **GPU チェック**\n",
        "#@markdown 割り当てられたGPUが**T4/P4/P100**のどれかであることを確認してください（K80はめっちゃ遅いので、お勧めしません）\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZA2zR-9_y2mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etzxXVZ_r-Nf",
        "cellView": "form"
      },
      "source": [
        "#@title **セットアップ**\n",
        "\n",
        "!pip install ftfy==5.8 transformers\n",
        "!pip install gputil ffpb \n",
        "\n",
        "# !apt-get -qq install ffmpeg\n",
        "work_dir = '/content/illustrip'\n",
        "import os\n",
        "os.makedirs(work_dir, exist_ok=True)\n",
        "%cd $work_dir\n",
        "\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import imageio\n",
        "import numpy as np\n",
        "import PIL\n",
        "from base64 import b64encode\n",
        "import shutil\n",
        "from easydict import EasyDict as edict\n",
        "a = edict()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from IPython.display import HTML, Image, display, clear_output\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "import ipywidgets as ipy\n",
        "from google.colab import output, files\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!pip install git+https://github.com/openai/CLIP.git --no-deps\n",
        "import clip\n",
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "!pip install kornia\n",
        "import kornia\n",
        "!pip install lpips\n",
        "import lpips\n",
        "!pip install PyWavelets==1.1.1\n",
        "!pip install git+https://github.com/fbcotter/pytorch_wavelets\n",
        "\n",
        "!pip install git+https://github.com/eps696/aphantasia\n",
        "from aphantasia.image import to_valid_rgb, fft_image, rfft2d_freqs, img2fft, pixel_image, un_rgb\n",
        "from aphantasia.utils import basename, file_list, img_list, img_read, txt_clean, plot_text, old_torch\n",
        "from aphantasia.utils import slice_imgs, derivat, pad_up_to, slerp, checkout, sim_func, latent_anima\n",
        "from aphantasia import transforms\n",
        "from aphantasia.progress_bar import ProgressIPy as ProgressBar\n",
        "\n",
        "%cd $work_dir\n",
        "!git clone https://github.com/cedro3/aphantasia.git --recursive\n",
        "work_dir = os.path.join(work_dir, 'aphantasia')\n",
        "%cd $work_dir\n",
        "from depth import depth\n",
        "# !wget https://github.com/eps696/aphantasia/blob/master/mask.jpg?raw=true -O mask.jpg\n",
        "depth_mask_file = os.path.join(work_dir, 'depth', 'mask.jpg')\n",
        "%cd /content\n",
        "\n",
        "def save_img(img, fname=None):\n",
        "  img = np.array(img)[:,:,:]\n",
        "  img = np.transpose(img, (1,2,0))  \n",
        "  img = np.clip(img*255, 0, 255).astype(np.uint8)\n",
        "  if fname is not None:\n",
        "    imageio.imsave(fname, np.array(img))\n",
        "    imageio.imsave('result.jpg', np.array(img))\n",
        "\n",
        "def makevid(seq_dir, size=None):\n",
        "  char_len = len(basename(img_list(seq_dir)[0]))\n",
        "  out_sequence = seq_dir + '/%0{}d.jpg'.format(char_len)\n",
        "  out_video = seq_dir + '.mp4'\n",
        "  print('.. generating video ..')\n",
        "  !ffmpeg -y -v warning -i $out_sequence -crf 18 $out_video\n",
        "\n",
        "%cd /content/illustrip/aphantasia/\n",
        "from function import *\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **ビデオの作成**\n",
        "\n",
        "# ============\n",
        "#     Setting   \n",
        "# ============\n",
        "%cd /content/illustrip/aphantasia/\n",
        "\n",
        "content = \"Pollinations is a system designed to produce joy in the form of honey.\" #@param {type:\"string\"}\n",
        "style = \"Illustration by Ernst Haeckel\" #@param {type:\"string\"}\n",
        "resume = False #@param {type:\"boolean\"}\n",
        "\n",
        "texts = [content]\n",
        "styles = [style]\n",
        "workname = txt_clean(content)[:44]\n",
        "\n",
        "if resume:\n",
        "  print('Upload file to resume from')\n",
        "  resumed = files.upload()\n",
        "  resumed = list(resumed.keys())  \n",
        "  size_opt(resumed[0]) \n",
        "  resumed_filename = resumed[0]\n",
        "\n",
        "assert len(texts) > 0 and len(texts[0]) > 0, 'No input text[s] found!'\n",
        "tempdir = os.path.join(work_dir, workname)\n",
        "os.makedirs(tempdir, exist_ok=True)\n",
        "print('main dir', tempdir)\n",
        "\n",
        "rotation = True #@param {type:\"boolean\"}\n",
        "three_d = True #@param {type:\"boolean\"}\n",
        "\n",
        "if rotation:\n",
        "  rotate = 0.8\n",
        "else:\n",
        "  rotate =0\n",
        "\n",
        "if three_d:\n",
        "  DepthStrength = 0.01\n",
        "else:\n",
        "  DepthStrength =0\n",
        "\n",
        "sideX = 640 #param {type:\"integer\"} \n",
        "sideY = 360 #param {type:\"integer\"} \n",
        "steps = 200 \n",
        "frame_step = 100 \n",
        "\n",
        "# Config\n",
        "method = 'RGB' \n",
        "model = 'ViT-B/32' \n",
        "\n",
        "# Setting\n",
        "align = 'overscan'\n",
        "colors = 2.3\n",
        "contrast = 1.2\n",
        "sharpness = -1.\n",
        "aug_noise = 0.\n",
        "smooth = False\n",
        "interpolate_topics = True\n",
        "style_power = 1.\n",
        "samples = 200\n",
        "save_step = 1\n",
        "learning_rate = 1.\n",
        "optimizer = 'adam'\n",
        "aug_transform = 'fast' #param [\"elastic\", \"custom\", \"fast\", \"none\"] {allow-input: true}\n",
        "similarity_function = 'mixed'\n",
        "macro = 0.4\n",
        "enforce = 0.\n",
        "expand = 0.\n",
        "zoom = 0.012 \n",
        "shift = 10 \n",
        "#rotate = 0.8 #@param {type:\"number\"} #0.8\n",
        "distort = 0.3\n",
        "animate_them = True\n",
        "sample_decrease = 1.\n",
        "#DepthStrength = 0.\n",
        "save_depth = False\n",
        "\n",
        "print(' loading CLIP model..')\n",
        "model_clip, _ = clip.load(model, jit=old_torch())\n",
        "modsize = model_clip.visual.input_resolution\n",
        "\n",
        "#%cd $work_dir\n",
        "clear_output()\n",
        "print(' using CLIP model', model)\n",
        "\n",
        "if resume:\n",
        "  img_in = imageio.imread(resumed_filename) \n",
        "  # params_tmp = torch.Tensor(img_in).permute(2,0,1).unsqueeze(0).float().cuda()\n",
        "  params_tmp = 3.3 * un_rgb(img_in, colors=2.)\n",
        "  sideY, sideX = img_in.shape[0], img_in.shape[1]\n",
        "else:\n",
        "  params_tmp = torch.randn(1, 3, sideY, sideX).cuda() # * 0.01\n",
        "\n",
        "\n",
        "# =================\n",
        "#     Add 3D depth   \n",
        "# =================\n",
        "\n",
        "depth_model = 'nyu' # @ param [\"nyu\",\"kitti\"]\n",
        "save_depth = False #param{type:\"boolean\"}\n",
        "MaskBlurAmt = 33 \n",
        "size = (sideY,sideX)\n",
        "\n",
        "%cd $work_dir\n",
        "\n",
        "if DepthStrength != 0:\n",
        "\n",
        "  if not os.path.exists(\"AdaBins_nyu.pt\"):\n",
        "    !gdown https://drive.google.com/uc?id=1lvyZZbC9NLcS8a__YPcUP7rDiIpbRpoF\n",
        "    if not os.path.exists('AdaBins_nyu.pt'):\n",
        "      !wget https://www.dropbox.com/s/tayczpcydoco12s/AdaBins_nyu.pt\n",
        "\n",
        "  if save_depth:\n",
        "    depthdir = os.path.join(tempdir, 'depth')\n",
        "    os.makedirs(depthdir, exist_ok=True)\n",
        "    print('depth dir', depthdir)\n",
        "  else:\n",
        "    depthdir = None\n",
        "\n",
        "  depth_infer, depth_mask = depth.init_adabins(size=size, model_path='AdaBins_nyu.pt', mask_path=depth_mask_file)\n",
        "\n",
        "  def depth_transform(img_t, depth_infer, depth_mask, size, depthX=0, scale=1., shift=[0,0], colors=1, depth_dir=None, save_num=0):\n",
        "    size2 = [s//2 for s in size]\n",
        "    if not isinstance(scale, float): scale = float(scale[0])\n",
        "    # d X/Y define the origin point of the depth warp, effectively a \"3D pan zoom\", [-1..1]\n",
        "    # plus = look ahead, minus = look aside\n",
        "    dX = 100. * shift[0] / size[1]\n",
        "    dY = 100. * shift[1] / size[0]\n",
        "    # dZ = movement direction: 1 away (zoom out), 0 towards (zoom in), 0.5 stay\n",
        "    dZ = 0.5 + 32. * (scale-1) \n",
        "    img = depth.depthwarp(img_t, depth_infer, depth_mask, size2, depthX, [dX,dY], dZ, save_path=depth_dir, save_num=save_num)\n",
        "    return img\n",
        "\n",
        "\n",
        "\n",
        "# =============\n",
        "#     Generate   \n",
        "# =============\n",
        "\n",
        "clear_output() ###\n",
        "\n",
        "if aug_transform == 'elastic':\n",
        "  trform_f = transforms.transforms_elastic\n",
        "  sample_decrease *= 0.95\n",
        "elif aug_transform == 'custom':\n",
        "  trform_f = transforms.transforms_custom\n",
        "  sample_decrease *= 0.95\n",
        "elif aug_transform == 'fast':\n",
        "  trform_f = transforms.transforms_fast\n",
        "  sample_decrease *= 0.95\n",
        "  print(' using fast aug transforms')\n",
        "else:\n",
        "  trform_f = transforms.normalize()\n",
        "\n",
        "if enforce != 0:\n",
        "  sample_decrease *= 0.5\n",
        "\n",
        "samples = int(samples * sample_decrease)\n",
        "print(' using %s method, %d samples, %s optimizer' % (method, samples, optimizer))\n",
        "print(' roate = '+str(rotate), 'DepthStrength = '+str(DepthStrength)) ###\n",
        "\n",
        "def enc_text(txt):\n",
        "  emb = model_clip.encode_text(clip.tokenize(txt).cuda()[:77])\n",
        "  return emb.detach().clone()\n",
        "\n",
        "# Encode inputs\n",
        "count = 0 # max count of texts and styles\n",
        "key_txt_encs = [enc_text(txt) for txt in texts]\n",
        "count = max(count, len(key_txt_encs))\n",
        "key_styl_encs = [enc_text(style) for style in styles]\n",
        "count = max(count, len(key_styl_encs))\n",
        "assert count > 0, \"No inputs found!\"\n",
        "\n",
        "glob_steps = count * steps # saving\n",
        "if glob_steps == frame_step: frame_step = glob_steps // 2 # otherwise no motion\n",
        "\n",
        "outpic = ipy.Output()\n",
        "outpic\n",
        "\n",
        "params_tmp = params_tmp.detach()\n",
        "\n",
        "# animation controls\n",
        "if animate_them:\n",
        "  m_scale = latent_anima([1], glob_steps, frame_step, uniform=True, cubic=True, start_lat=[-0.3])\n",
        "  m_scale = 1 + (m_scale + 0.3) * zoom # only zoom in\n",
        "  m_shift = latent_anima([2], glob_steps, frame_step, uniform=True, cubic=True, start_lat=[0.5,0.5])\n",
        "  m_angle = latent_anima([1], glob_steps, frame_step, uniform=True, cubic=True, start_lat=[0.5])\n",
        "  m_shear = latent_anima([1], glob_steps, frame_step, uniform=True, cubic=True, start_lat=[0.5])\n",
        "  m_shift = (m_shift-0.5) * shift   * abs(m_scale-1.) / zoom\n",
        "  m_angle = (m_angle-0.5) * rotate  * abs(m_scale-1.) / zoom\n",
        "  m_shear = (m_shear-0.5) * distort * abs(m_scale-1.) / zoom\n",
        "\n",
        "def get_encs(encs, num):\n",
        "  cnt = len(encs)\n",
        "  if cnt == 0: return []\n",
        "  enc_1 = encs[min(num,   cnt-1)]\n",
        "  enc_2 = encs[min(num+1, cnt-1)]\n",
        "  return slerp(enc_1, enc_2, steps)\n",
        "\n",
        "def frame_transform(img, size, angle, shift, scale, shear):\n",
        "  if old_torch(): # 1.7.1\n",
        "    img = T.functional.affine(img, angle, shift, scale, shear, fillcolor=0, resample=PIL.Image.BILINEAR)\n",
        "    img = T.functional.center_crop(img, size)\n",
        "    img = pad_up_to(img, size)\n",
        "  else: # 1.8+\n",
        "    img = T.functional.affine(img, angle, shift, scale, shear, fill=0, interpolation=T.InterpolationMode.BILINEAR)\n",
        "    img = T.functional.center_crop(img, size) # on 1.8+ also pads\n",
        "  return img\n",
        "\n",
        "prev_enc = 0\n",
        "def process(num):\n",
        "  global params_tmp, opt_state, params, image_f, optimizer, pbar\n",
        "\n",
        "  if interpolate_topics:\n",
        "    txt_encs  = get_encs(key_txt_encs,  num)\n",
        "    styl_encs = get_encs(key_styl_encs, num)\n",
        "  else:\n",
        "    txt_encs  = [key_txt_encs[min(num,  len(key_txt_encs)-1)][0]]  * steps if len(key_txt_encs)  > 0 else []\n",
        "    styl_encs = [key_styl_encs[min(num, len(key_styl_encs)-1)][0]] * steps if len(key_styl_encs) > 0 else []\n",
        "\n",
        "  if len(texts)  > 0: print(' ref text: ',  texts[min(num, len(texts)-1)][:80])\n",
        "  if len(styles) > 0: print(' ref style: ', styles[min(num, len(styles)-1)][:80])\n",
        "\n",
        "  for ii in range(steps):\n",
        "    glob_step = num * steps + ii # saving/transforming\n",
        "\n",
        "    # get encoded inputs\n",
        "    txt_enc  = txt_encs[ii % len(txt_encs)].unsqueeze(0)   if len(txt_encs)  > 0 else None\n",
        "    styl_enc = styl_encs[ii % len(styl_encs)].unsqueeze(0) if len(styl_encs) > 0 else None\n",
        "    \n",
        "    ### animation: transform frame, reload params\n",
        "\n",
        "    h, w = sideY, sideX\n",
        "    \n",
        "    # transform frame for motion\n",
        "    scale =       m_scale[glob_step]    if animate_them else 1-zoom\n",
        "    trans = tuple(m_shift[glob_step])   if animate_them else [0, shift]\n",
        "    angle =       m_angle[glob_step][0] if animate_them else rotate\n",
        "    shear =       m_shear[glob_step][0] if animate_them else distort\n",
        "\n",
        "    if DepthStrength != 0:\n",
        "      params_tmp = depth_transform(params_tmp, depth_infer, depth_mask, size, DepthStrength, scale, trans, colors, depthdir, glob_step)\n",
        "    params_tmp = frame_transform(params_tmp, (h,w), angle, trans, scale, shear)\n",
        "    params, image_f, _ = pixel_image([1,3,h,w], resume=params_tmp)\n",
        "    img_tmp = None\n",
        "\n",
        "    image_f = to_valid_rgb(image_f, colors=colors)\n",
        "    del img_tmp\n",
        "\n",
        "    if optimizer.lower() == 'adamw':\n",
        "      optimr = torch.optim.AdamW(params, learning_rate, weight_decay=0.01, amsgrad=True)\n",
        "    elif optimizer.lower() == 'adamw_custom':\n",
        "      optimr = torch.optim.AdamW(params, learning_rate, weight_decay=0.01, amsgrad=True, betas=(.0,.999))\n",
        "    elif optimizer.lower() == 'adam':\n",
        "      optimr = torch.optim.Adam(params, learning_rate)\n",
        "    else: # adam_custom\n",
        "      optimr = torch.optim.Adam(params, learning_rate, betas=(.0,.999))\n",
        "\n",
        "    if smooth is True and num + ii > 0:\n",
        "      optimr.load_state_dict(opt_state)\n",
        "\n",
        "    ### optimization\n",
        "\n",
        "    for ss in range(save_step):\n",
        "      loss = 0\n",
        "\n",
        "      noise = aug_noise * (torch.rand(1, 1, *params[0].shape[2:4], 1)-0.5).cuda() if aug_noise > 0 else 0.\n",
        "      img_out = image_f(noise, fixcontrast=resume)\n",
        "      img_sliced = slice_imgs([img_out], samples, modsize, trform_f, align, macro)[0]\n",
        "      out_enc = model_clip.encode_image(img_sliced)\n",
        "\n",
        "      #if method == 'RGB': # empirical hack\n",
        "      loss += abs(img_out.mean((2,3)) - 0.45).mean() # fix brightness\n",
        "      loss += abs(img_out.std((2,3)) - 0.17).sum() # fix contrast\n",
        "\n",
        "      if txt_enc is not None:\n",
        "        loss -= sim_func(txt_enc, out_enc, similarity_function)\n",
        "      if styl_enc is not None:\n",
        "        loss -= style_power * sim_func(styl_enc, out_enc, similarity_function)\n",
        "      if sharpness != 0: # mode = scharr|sobel|naive\n",
        "        loss -= sharpness * derivat(img_out, mode='naive')\n",
        "        # loss -= sharpness * derivat(img_sliced, mode='scharr')\n",
        "      if enforce != 0:\n",
        "        img_sliced = slice_imgs([image_f(noise, fixcontrast=resume)], samples, modsize, trform_f, align, macro)[0]\n",
        "        out_enc2 = model_clip.encode_image(img_sliced)\n",
        "        loss -= enforce * sim_func(out_enc, out_enc2, similarity_function)\n",
        "        del out_enc2; torch.cuda.empty_cache()\n",
        "      if expand > 0:\n",
        "        global prev_enc\n",
        "        if ii > 0:\n",
        "          loss += expand * sim_func(prev_enc, out_enc, similarity_function)\n",
        "        prev_enc = out_enc.detach().clone()\n",
        "      del img_out, img_sliced, out_enc; torch.cuda.empty_cache()\n",
        "\n",
        "      optimr.zero_grad()\n",
        "      loss.backward()\n",
        "      optimr.step()\n",
        "    \n",
        "    ### save params & frame\n",
        "\n",
        "    params_tmp = params[0].detach().clone()\n",
        "    if smooth is True:\n",
        "      opt_state = optimr.state_dict()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      img_t = image_f(contrast=contrast, fixcontrast=resume)[0].permute(1,2,0)\n",
        "      img_np = torch.clip(img_t*255, 0, 255).cpu().numpy().astype(np.uint8)\n",
        "    imageio.imsave(os.path.join(tempdir, '%05d.jpg' % glob_step), img_np, quality=95)\n",
        "    shutil.copy(os.path.join(tempdir, '%05d.jpg' % glob_step), 'result.jpg')\n",
        "    outpic.clear_output()\n",
        "    with outpic:\n",
        "      display(Image('result.jpg'))\n",
        "    del img_t\n",
        "    pbar.upd()\n",
        "\n",
        "  params_tmp = params[0].detach().clone()\n",
        "\n",
        "outpic = ipy.Output()\n",
        "outpic\n",
        "\n",
        "pbar = ProgressBar(glob_steps)\n",
        "for i in range(count):\n",
        "  process(i)\n",
        "\n",
        "makevid(tempdir)\n",
        "\n",
        "clear_output() \n",
        "print('waiting for play movie ...')\n",
        "display_mp4(tempdir + '.mp4')\n",
        "files.download(tempdir + '.mp4')"
      ],
      "metadata": {
        "id": "Zw0zcdGAsyNx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}